{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc1da461",
   "metadata": {},
   "source": [
    "# Data & Algoritma Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28d2c3",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac478f9",
   "metadata": {},
   "source": [
    "### üìä Nama Dataset\n",
    "liputan6_data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c5914",
   "metadata": {},
   "source": [
    "### üåç Languages\n",
    "- Indonesian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f459f8",
   "metadata": {},
   "source": [
    "### üß© Data Structure\n",
    "|Nama Kolom|Tipe Data|\n",
    "|----|--------|\n",
    "|`id`|`string`|\n",
    "|`url`|`string`|\n",
    "|`clean_article`|`string`|\n",
    "|`clean_summary`|`string`|\n",
    "|`extractive_summary`|`string`|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00beada",
   "metadata": {},
   "source": [
    "### Data Instances\n",
    "|Nama Kolom|Contoh Data|\n",
    "|----------|-----------|\n",
    "|`id`|26408|\n",
    "|`url`|https://www.liputan6.com/news/read/26408/pbb-siap-membantu-penyelesaian-konflik-ambon|\n",
    "|`clean_article`|Liputan6.com, Ambon: Partai Bulan Bintang wilayah Maluku bertekad membantu pemerintah menyelesaikan konflik di provinsi tersebut. Syaratnya, penanganan penyelesaian konflik Maluku harus dimulai dari awal kerusuhan, yakni 19 Januari 1999. Demikian hasil Musyawarah Wilayah I PBB Maluku yang dimulai Sabtu pekan silam dan berakhir Senin (31/12) di Ambon. Menurut seorang fungsionaris PBB Ridwan Hasan, persoalan di Maluku bisa selesai asalkan pemerintah dan aparat keamanan serius menangani setiap persoalan di Maluku secara komprehensif dan bijaksana. Itulah sebabnya, PBB wilayah Maluku akan menjadikan penyelesaian konflik sebagai agenda utama partai. PBB Maluku juga akan mendukung penegakan hukum secara terpadu dan tanpa pandang bulu. Siapa saja yang melanggar hukum harus ditindak. Ridwan berharap, Ketua PBB Maluku yang baru, Ali Fauzi, dapat menindak lanjuti agenda politik partai yang telah diamanatkan dan mau mendukung penegakan hukum di Maluku. (ULF/Sahlan Heluth).|\n",
    "|`clean_summary`|Konflik Ambon telah berlangsung selama tiga tahun. Partai Bulan Bintang wilayah Maluku siap membantu pemerintah menyelesaikan kasus di provinsi tersebut.|\n",
    "|`extractive_summary`|Liputan6.com, Ambon: Partai Bulan Bintang wilayah Maluku bertekad membantu pemerintah menyelesaikan konflik di provinsi tersebut. Siapa saja yang melanggar hukum harus ditindak.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acce5cb",
   "metadata": {},
   "source": [
    "### Data Fields\n",
    "|Nama Kolom|Keterangan|\n",
    "|----------|----------|\n",
    "|`id`|Kolom id unique|\n",
    "|`url`|URL Article|\n",
    "|`clean_article`|Isi original article|\n",
    "|`clean_summary`|Ringkasan Abstract|\n",
    "|`extractive_summary`|Ringkasan Ekstractif|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a2107",
   "metadata": {},
   "source": [
    "## Algoritma Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2557920",
   "metadata": {},
   "source": [
    "# Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28817c3e",
   "metadata": {},
   "source": [
    "### Load Dataset dan Convert Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4761a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d917f740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>clean_article</th>\n",
       "      <th>clean_summary</th>\n",
       "      <th>extractive_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>https://www.liputan6.com/news/read/100000/yudh...</td>\n",
       "      <td>[['Liputan6', '.', 'com', ',', 'Jakarta', ':',...</td>\n",
       "      <td>[['Menurut', 'Presiden', 'Susilo', 'Bambang', ...</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100002</td>\n",
       "      <td>https://www.liputan6.com/news/read/100002/jepa...</td>\n",
       "      <td>[['Liputan6', '.', 'com', ',', 'Jakarta', ':',...</td>\n",
       "      <td>[['Pada', 'masa', 'silam', 'Jepang', 'terlalu'...</td>\n",
       "      <td>[2, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100003</td>\n",
       "      <td>https://www.liputan6.com/news/read/100003/pulu...</td>\n",
       "      <td>[['Liputan6', '.', 'com', ',', 'Kutai', ':', '...</td>\n",
       "      <td>[['Puluhan', 'hektare', 'areal', 'persawahan',...</td>\n",
       "      <td>[1, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100004</td>\n",
       "      <td>https://www.liputan6.com/news/read/100004/pres...</td>\n",
       "      <td>[['Liputan6', '.', 'com', ',', 'Jakarta', ':',...</td>\n",
       "      <td>[['Sekjen', 'PBB', 'Kofi', 'Annan', 'memuji', ...</td>\n",
       "      <td>[2, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100005</td>\n",
       "      <td>https://www.liputan6.com/news/read/100005/warg...</td>\n",
       "      <td>[['Liputan6', '.', 'com', ',', 'Solok', ':', '...</td>\n",
       "      <td>[['Untuk', 'mempercepat', 'pelaksanaan', 'bela...</td>\n",
       "      <td>[0, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                                url  \\\n",
       "0  100000  https://www.liputan6.com/news/read/100000/yudh...   \n",
       "1  100002  https://www.liputan6.com/news/read/100002/jepa...   \n",
       "2  100003  https://www.liputan6.com/news/read/100003/pulu...   \n",
       "3  100004  https://www.liputan6.com/news/read/100004/pres...   \n",
       "4  100005  https://www.liputan6.com/news/read/100005/warg...   \n",
       "\n",
       "                                       clean_article  \\\n",
       "0  [['Liputan6', '.', 'com', ',', 'Jakarta', ':',...   \n",
       "1  [['Liputan6', '.', 'com', ',', 'Jakarta', ':',...   \n",
       "2  [['Liputan6', '.', 'com', ',', 'Kutai', ':', '...   \n",
       "3  [['Liputan6', '.', 'com', ',', 'Jakarta', ':',...   \n",
       "4  [['Liputan6', '.', 'com', ',', 'Solok', ':', '...   \n",
       "\n",
       "                                       clean_summary extractive_summary  \n",
       "0  [['Menurut', 'Presiden', 'Susilo', 'Bambang', ...             [0, 1]  \n",
       "1  [['Pada', 'masa', 'silam', 'Jepang', 'terlalu'...             [2, 3]  \n",
       "2  [['Puluhan', 'hektare', 'areal', 'persawahan',...             [1, 5]  \n",
       "3  [['Sekjen', 'PBB', 'Kofi', 'Annan', 'memuji', ...             [2, 5]  \n",
       "4  [['Untuk', 'mempercepat', 'pelaksanaan', 'bela...             [0, 2]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/train_data.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58072543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 193883 entries, 0 to 193882\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   id                  193883 non-null  int64 \n",
      " 1   url                 193883 non-null  object\n",
      " 2   clean_article       193883 non-null  object\n",
      " 3   clean_summary       193883 non-null  object\n",
      " 4   extractive_summary  193883 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 7.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6abbca",
   "metadata": {},
   "source": [
    "## Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53198aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SUWAAAN\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf73b57",
   "metadata": {},
   "source": [
    "### Memuat Data dan Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1129046b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memuat data...\n",
      "Membersihkan data...\n",
      "Membagi data menjadi set Training (60%), Validasi (20%), dan Testing (20%)...\n",
      "Jumlah artikel untuk Training: 6582\n",
      "Jumlah artikel untuk Validasi: 2195\n",
      "Jumlah artikel untuk Testing: 2195\n",
      "\n",
      "Mengubah data menjadi format per-kalimat...\n",
      "Jumlah kalimat di data Training: 76399\n",
      "Jumlah kalimat di data Validasi: 25819\n",
      "Jumlah kalimat di data Testing: 26240\n"
     ]
    }
   ],
   "source": [
    "# Konversi string yang berisi list menjadi list\n",
    "def parse_list_string(s):\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "# Load dataset\n",
    "print(\"Memuat data...\")\n",
    "df_train = pd.read_csv('../data/train2_data.csv')\n",
    "\n",
    "# Cleaning data\n",
    "print(\"Membersihkan data...\")\n",
    "df_train['clean_article'] = df_train['clean_article'].apply(parse_list_string)\n",
    "df_train['extractive_summary'] = df_train['extractive_summary'].apply(parse_list_string)\n",
    "\n",
    "\n",
    "print(\"Membagi data menjadi set Training (60%), Validasi (20%), dan Testing (20%)...\")\n",
    "\n",
    "# Ambil data validasi 20% dari data training\n",
    "df_train_val, df_test_articles = train_test_split(\n",
    "    df_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Ambil data testing 25% dari data training\n",
    "df_train_articles, df_val_articles = train_test_split(\n",
    "    df_train_val,\n",
    "    test_size=0.25,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Jumlah artikel untuk Training: {len(df_train_articles)}\")\n",
    "print(f\"Jumlah artikel untuk Validasi: {len(df_val_articles)}\")\n",
    "print(f\"Jumlah artikel untuk Testing: {len(df_test_articles)}\")\n",
    "\n",
    "\n",
    "# Fungsi untuk mengubah format dari per-artikel menjadi per-kalimat\n",
    "def create_sentence_dataframe(df):\n",
    "    processed_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        article_sentences_tokens = row['clean_article']\n",
    "        summary_indices = set(row['extractive_summary'])\n",
    "        sentences_as_strings = [\" \".join(sent) for sent in article_sentences_tokens]\n",
    "        for i, sent_text in enumerate(sentences_as_strings):\n",
    "            processed_data.append({\n",
    "                'sentence': sent_text,\n",
    "                'label': 1 if i in summary_indices else 0,\n",
    "            })\n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "# Buat DataFrame per-kalimat untuk setiap set secara terpisah\n",
    "print(\"\\nMengubah data menjadi format per-kalimat...\")\n",
    "train_sentences_df = create_sentence_dataframe(df_train_articles)\n",
    "val_sentences_df = create_sentence_dataframe(df_val_articles)\n",
    "test_sentences_df = create_sentence_dataframe(df_test_articles)\n",
    "\n",
    "print(f\"Jumlah kalimat di data Training: {len(train_sentences_df)}\")\n",
    "print(f\"Jumlah kalimat di data Validasi: {len(val_sentences_df)}\")\n",
    "print(f\"Jumlah kalimat di data Testing: {len(test_sentences_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63cc7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi Kelas pada Data Training:\n",
      "label\n",
      "0    62562\n",
      "1    13837\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribusi Kelas dalam Persentase:\n",
      "label\n",
      "0    81.888506\n",
      "1    18.111494\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Hitung jumlah setiap nilai unik di kolom 'label'\n",
    "class_distribution = train_sentences_df['label'].value_counts()\n",
    "\n",
    "print(\"Distribusi Kelas pada Data Training:\")\n",
    "print(class_distribution)\n",
    "\n",
    "# Jika ingin melihat dalam bentuk persentase\n",
    "print(\"\\nDistribusi Kelas dalam Persentase:\")\n",
    "class_percentage = train_sentences_df['label'].value_counts(normalize=True) * 100\n",
    "print(class_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbd9a94",
   "metadata": {},
   "source": [
    "### Tokenisasi dengan IndoBERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a8086cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memuat tokenizer dari indobenchmark/indobert-base-p1...\n",
      "\n",
      "Melakukan tokenisasi pada semua dataset...\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'indobenchmark/indobert-base-p1'\n",
    "MAX_LEN = 128\n",
    "\n",
    "print(f\"\\nMemuat tokenizer dari {MODEL_NAME}...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def convert_to_bert_input(sentences, tokenizer, max_len):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        sentences.tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "print(\"\\nMelakukan tokenisasi pada semua dataset...\")\n",
    "# Tokenisasi semua set data secara terpisah\n",
    "X_train_tokens = convert_to_bert_input(train_sentences_df['sentence'], tokenizer, MAX_LEN)\n",
    "X_val_tokens = convert_to_bert_input(val_sentences_df['sentence'], tokenizer, MAX_LEN)\n",
    "X_test_tokens = convert_to_bert_input(test_sentences_df['sentence'], tokenizer, MAX_LEN)\n",
    "\n",
    "# Ambil label untuk setiap set secara terpisah\n",
    "y_train = train_sentences_df['label'].values\n",
    "y_val = val_sentences_df['label'].values\n",
    "y_test = test_sentences_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d19d4ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Menghitung bobot kelas untuk mengatasi data tidak seimbang...\n",
      "Bobot Kelas yang akan digunakan: {0: 0.6105862983919952, 1: 2.760677892606779}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMenghitung bobot kelas untuk mengatasi data tidak seimbang...\")\n",
    "# Hitung bobotnya\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "# Ubah menjadi format dictionary yang bisa dibaca Keras\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(f\"Bobot Kelas yang akan digunakan: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f6f9a5",
   "metadata": {},
   "source": [
    "### MEMBANGUN DAN MELATIH MODEL IndoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68a92dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dc8f908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memuat pre-trained IndoBERT model...\n",
      "WARNING:tensorflow:From C:\\Users\\SUWAAAN\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at indobenchmark/indobert-base-p1 were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at indobenchmark/indobert-base-p1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)      [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer  [(None, 128)]                0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel  TFBaseModelOutputWithPooli   1244413   ['input_ids[0][0]',           \n",
      " )                           ngAndCrossAttentions(last_   44         'attention_mask[0][0]']      \n",
      "                             hidden_state=(None, 128, 7                                           \n",
      "                             68),                                                                 \n",
      "                              pooler_output=(None, 768)                                           \n",
      "                             , past_key_values=None, hi                                           \n",
      "                             dden_states=None, attentio                                           \n",
      "                             ns=None, cross_attentions=                                           \n",
      "                             None)                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  (None, 768)                  0         ['tf_bert_model[0][0]']       \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)        (None, 768)                  0         ['tf.__operators__.getitem[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1)                    769       ['dropout_37[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124442113 (474.71 MB)\n",
      "Trainable params: 124442113 (474.71 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Memulai training model...\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From C:\\Users\\SUWAAAN\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "1194/1194 [==============================] - ETA: 0s - loss: 0.6563 - accuracy: 0.6352 - precision: 0.2717 - recall: 0.6036 \n",
      "Epoch 1: val_loss improved from inf to 0.63119, saving model to indobert_summarizer_best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUWAAAN\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\engine\\training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1194/1194 [==============================] - 14221s 12s/step - loss: 0.6563 - accuracy: 0.6352 - precision: 0.2717 - recall: 0.6036 - val_loss: 0.6312 - val_accuracy: 0.6632 - val_precision: 0.2949 - val_recall: 0.6413\n",
      "Epoch 2/5\n",
      "1194/1194 [==============================] - ETA: 0s - loss: 0.5948 - accuracy: 0.6818 - precision: 0.3200 - recall: 0.6728\n",
      "Epoch 2: val_loss improved from 0.63119 to 0.59770, saving model to indobert_summarizer_best_model.h5\n",
      "1194/1194 [==============================] - 12938s 11s/step - loss: 0.5948 - accuracy: 0.6818 - precision: 0.3200 - recall: 0.6728 - val_loss: 0.5977 - val_accuracy: 0.6835 - val_precision: 0.3065 - val_recall: 0.6156\n",
      "Epoch 3/5\n",
      "1194/1194 [==============================] - ETA: 0s - loss: 0.5208 - accuracy: 0.7301 - precision: 0.3785 - recall: 0.7635\n",
      "Epoch 3: val_loss did not improve from 0.59770\n",
      "1194/1194 [==============================] - 13000s 11s/step - loss: 0.5208 - accuracy: 0.7301 - precision: 0.3785 - recall: 0.7635 - val_loss: 0.7267 - val_accuracy: 0.5858 - val_precision: 0.2619 - val_recall: 0.7298\n",
      "Epoch 4/5\n",
      "1194/1194 [==============================] - ETA: 0s - loss: 0.3882 - accuracy: 0.8041 - precision: 0.4775 - recall: 0.8620\n",
      "Epoch 4: val_loss did not improve from 0.59770\n",
      "1194/1194 [==============================] - 12809s 11s/step - loss: 0.3882 - accuracy: 0.8041 - precision: 0.4775 - recall: 0.8620 - val_loss: 0.8003 - val_accuracy: 0.6276 - val_precision: 0.2694 - val_recall: 0.6378\n",
      "Epoch 4: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Training selesai.\n"
     ]
    }
   ],
   "source": [
    "def build_model(bert_model, max_len=128):\n",
    "    input_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    sequence_output = bert_model(input_ids, attention_mask=attention_mask)[0]\n",
    "    cls_token_output = sequence_output[:, 0, :]\n",
    "    x = tf.keras.layers.Dropout(0.2)(cls_token_output)\n",
    "    output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "print(\"\\nMemuat pre-trained IndoBERT model...\")\n",
    "bert_model = TFBertModel.from_pretrained(MODEL_NAME)\n",
    "model = build_model(bert_model, max_len=MAX_LEN)\n",
    "model.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "metrics = [\n",
    "    tf.keras.metrics.BinaryAccuracy('accuracy'),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall')\n",
    "]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='indobert_summarizer_best_model.h5',\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5 \n",
    "\n",
    "print(\"\\nMemulai training model...\")\n",
    "\n",
    "# Konversi input menjadi format dictionary yang bisa dibaca model\n",
    "train_input_dict = {'input_ids': tf.constant(X_train_tokens['input_ids']), 'attention_mask': tf.constant(X_train_tokens['attention_mask'])}\n",
    "val_input_dict = {'input_ids': tf.constant(X_val_tokens['input_ids']), 'attention_mask': tf.constant(X_val_tokens['attention_mask'])}\n",
    "\n",
    "history = model.fit(\n",
    "    train_input_dict,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(val_input_dict, y_val),\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    class_weight=class_weight_dict\n",
    ")\n",
    "\n",
    "print(\"Training selesai.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14406d93",
   "metadata": {},
   "source": [
    "### Evaluasi Model pada Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1147b4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memuat model terbaik dari checkpoint untuk evaluasi...\n",
      "WARNING:tensorflow:From C:\\Users\\SUWAAAN\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\backend.py:1400: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "\n",
      "Melakukan evaluasi pada Test Set (data yang belum pernah dilihat)...\n",
      "820/820 [==============================] - 1311s 2s/step\n",
      "\n",
      "Classification Report (Test Set):\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "Bukan Ringkasan (0)       0.92      0.48      0.63     21610\n",
      "      Ringkasan (1)       0.25      0.80      0.38      4630\n",
      "\n",
      "           accuracy                           0.54     26240\n",
      "          macro avg       0.58      0.64      0.50     26240\n",
      "       weighted avg       0.80      0.54      0.59     26240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMemuat model terbaik dari checkpoint untuk evaluasi...\")\n",
    "\n",
    "best_model = tf.keras.models.load_model(\n",
    "    'indobert_summarizer_best_model-melihat_class_weight.h5', \n",
    "    custom_objects={\"TFBertModel\": TFBertModel}\n",
    ")\n",
    "\n",
    "threshold = 0.4\n",
    "\n",
    "# Siapkan data test\n",
    "test_input_dict = {'input_ids': tf.constant(X_test_tokens['input_ids']), 'attention_mask': tf.constant(X_test_tokens['attention_mask'])}\n",
    "\n",
    "print(\"\\nMelakukan evaluasi pada Test Set (data yang belum pernah dilihat)...\")\n",
    "y_pred_proba = best_model.predict(test_input_dict)\n",
    "y_pred = (y_pred_proba > threshold).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Bukan Ringkasan (0)', 'Ringkasan (1)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "203d2964",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_article_data = df_test_articles['clean_article'].iloc[0]\n",
    "sample_sentences = [\" \".join(sent) for sent in sample_article_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8acddc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Liputan6', '.', 'com', ',', 'Jakarta', ':', 'Pengamat', 'politik', 'Andi', 'Malarangeng', ',', 'baru-baru', 'ini', ',', 'menilai', 'pembaharuan', 'kode', 'etik', 'DPR', 'tidak', 'akan', 'berarti', 'banyak', 'dalam', 'mengubah', 'etika', 'berpolitik', 'para', 'anggota', 'Dewan', '.'], ['Sebab', ',', 'pengaturan', 'masalah', 'etika', 'belum', 'ditetapkan', 'dalam', 'Undang-undang', '.'], ['Menurut', 'Andi', ',', 'muatan', 'kode', 'etik', 'DPR', 'yang', 'akan', 'disampaikan', 'pekan', 'ini', 'masih', 'terlihat', 'samar', '.'], ['Selain', 'itu', ',', 'hubungan', 'antara', 'Pasal', 'Satu', 'dan', 'lainnya', 'tak', 'memiliki', 'konsekuensi', 'yang', 'jelas', '.'], ['Akibatnya', ',', 'kode', 'etik', 'tersebut', 'hanya', 'akan', 'menimbulkan', 'perdebatan', '.'], ['Bahkan', ',', 'tak', 'layak', 'menjadi', 'acuan', 'pokok', 'kedisiplinan', 'dan', 'etika', 'politik', 'para', 'anggota', 'DPR', ',', 'terutama', 'soal', 'batasan', 'hibah', ',', 'sangsi', 'kedisiplinan', 'hingga', 'pembentukan', 'Dewan', 'Kehormatan', 'Angota', 'DPR', '.'], ['Karena', 'itu', ',', 'Andi', 'menilai', 'sudah', 'seharusnya', 'kode', 'etik', 'tersebut', 'dapat', 'dilepaskan', 'dari', 'pengaturan', 'UU', '.'], ['Sebab', ',', 'etika', 'DPR', 'dan', 'pejabat', 'negara', ',', 'hingga', 'kini', ',', 'masih', 'belum', 'diatur', 'dalam', 'UU', '.'], ['Menanggapi', 'hal', 'tersebut', ',', 'Ketua', 'Komisi', 'II', 'DPR', 'Teras', 'Narang', 'menyatakan', ',', 'dalam', 'penegakan', 'kode', 'etik', 'anggota', 'DPR', 'seharusnya', 'juga', 'melibatkan', 'pimpinan', 'partai', 'politik', '.'], ['Sebab', ',', 'para', 'anggota', 'DPR', 'juga', 'terkait', 'dengan', 'kode', 'etik', 'partai', '.'], ['Rencananya', ',', 'awal', 'Oktober', 'ini', ',', 'DPR', 'akan', 'membahas', 'pembaharuan', 'kode', 'etik', 'tersebut', '.'], ['Dalam', 'pembaharuan', ',', 'akan', 'dimuat', 'pasal-pasal', 'yang', 'mengikat', 'kedisiplinan', 'para', 'anggota', 'Dewan', 'dengan', 'ancaman', 'hingga', 'sangsi', ',', 'termasuk', 'soal', 'hibah', 'dan', 'pembentukan', 'Dewan', 'Kehormatan', 'Anggota', 'DPR', '.', '(', 'PIN/Mahmud', 'dan', 'Yosef', 'HL', ')', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(sample_article_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a2c161b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Liputan6 . com , Jakarta : Pengamat politik Andi Malarangeng , baru-baru ini , menilai pembaharuan kode etik DPR tidak akan berarti banyak dalam mengubah etika berpolitik para anggota Dewan .', 'Sebab , pengaturan masalah etika belum ditetapkan dalam Undang-undang .', 'Menurut Andi , muatan kode etik DPR yang akan disampaikan pekan ini masih terlihat samar .', 'Selain itu , hubungan antara Pasal Satu dan lainnya tak memiliki konsekuensi yang jelas .', 'Akibatnya , kode etik tersebut hanya akan menimbulkan perdebatan .', 'Bahkan , tak layak menjadi acuan pokok kedisiplinan dan etika politik para anggota DPR , terutama soal batasan hibah , sangsi kedisiplinan hingga pembentukan Dewan Kehormatan Angota DPR .', 'Karena itu , Andi menilai sudah seharusnya kode etik tersebut dapat dilepaskan dari pengaturan UU .', 'Sebab , etika DPR dan pejabat negara , hingga kini , masih belum diatur dalam UU .', 'Menanggapi hal tersebut , Ketua Komisi II DPR Teras Narang menyatakan , dalam penegakan kode etik anggota DPR seharusnya juga melibatkan pimpinan partai politik .', 'Sebab , para anggota DPR juga terkait dengan kode etik partai .', 'Rencananya , awal Oktober ini , DPR akan membahas pembaharuan kode etik tersebut .', 'Dalam pembaharuan , akan dimuat pasal-pasal yang mengikat kedisiplinan para anggota Dewan dengan ancaman hingga sangsi , termasuk soal hibah dan pembentukan Dewan Kehormatan Anggota DPR . ( PIN/Mahmud dan Yosef HL ) .']\n"
     ]
    }
   ],
   "source": [
    "print(sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e3df4",
   "metadata": {},
   "source": [
    "### INFERENCE (MEMBUAT RINGKASAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17a69007",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_article_data = df_test_articles['clean_article'].iloc[1]\n",
    "sample_sentences = [\" \".join(sent) for sent in sample_article_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a5688f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Artikel Asli (5 kalimat pertama) ---\n",
      "- Liputan6 . com , Jakarta : Pemerintah harus menyelesaikan Kasus Aceh secara konsisten dan komprehensif .\n",
      "- Pertumbuhan separatis Gerakan Aceh Merdeka dan rasa ketidakadilan antarwarga dinilai penyebab bermacam masalah di Serambi Mekah itu .\n",
      "- Hal itu mengemuka dalam Rapat Kerja antara Komisi I DPR dan Panglima TNI Laksamana Widodo A . S .\n",
      "- di Jakarta , Senin ( 3/12 ) .\n",
      "- Widodo menilai , upaya dialog yang dilakukan selama ini tak sempurna .\n",
      "\n",
      "--- Ringkasan yang Dihasilkan Model (dengan Threshold = 0.4) ---\n",
      "1/1 [==============================] - 1s 545ms/step\n",
      "Liputan6 . com , Jakarta : Pemerintah harus menyelesaikan Kasus Aceh secara konsisten dan komprehensif . Pertumbuhan separatis Gerakan Aceh Merdeka dan rasa ketidakadilan antarwarga dinilai penyebab bermacam masalah di Serambi Mekah itu . Widodo menilai , upaya dialog yang dilakukan selama ini tak sempurna . Sebab , komponen yang hadir tidak mencerminkan keseluruhan aspirasi rakyat Aceh . Sementara dialog bilateral dengan GAM juga tak menemukan solusi jelas . Karena itu , Widodo meminta , semua pihak mau duduk sejajar untuk menghentikan konflik . Sejauh ini , langkah pemerintah dinilai sudah optimal . Satu di antaranya dengan pemberlakukan Undang-Undang Nanggroe Aceh Darussalam .\n"
     ]
    }
   ],
   "source": [
    "def summarize_article_with_threshold(article_sentences, model, tokenizer, max_len, threshold=threshold):\n",
    "    inputs = tokenizer.batch_encode_plus(\n",
    "        article_sentences,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf',\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    input_dict = {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask']}\n",
    "    \n",
    "    # Dapatkan probabilitas untuk setiap kalimat\n",
    "    probabilities = model.predict(input_dict).flatten()\n",
    "    \n",
    "    # Saring kalimat berdasarkan threshold\n",
    "    summary_indices = []\n",
    "    for i, p in enumerate(probabilities):\n",
    "        if p > threshold:\n",
    "            summary_indices.append(i)\n",
    "    \n",
    "    summary = \" \".join([article_sentences[i] for i in summary_indices])\n",
    "    \n",
    "    # Jika tidak ada kalimat yang lolos, kembalikan kalimat dengan skor tertinggi\n",
    "    if not summary:\n",
    "        print(\"Tidak ada kalimat yang melewati threshold, mengambil 1 kalimat terbaik.\")\n",
    "        best_index = np.argmax(probabilities)\n",
    "        summary = article_sentences[best_index]\n",
    "        \n",
    "    return summary\n",
    "\n",
    "\n",
    "print(\"--- Artikel Asli (5 kalimat pertama) ---\")\n",
    "for sent in sample_sentences[:5]:\n",
    "    print(\"- \" + sent)\n",
    "\n",
    "print(f\"\\n--- Ringkasan yang Dihasilkan Model (dengan Threshold = {threshold}) ---\")\n",
    "generated_summary_threshold = summarize_article_with_threshold(\n",
    "    sample_sentences, \n",
    "    best_model, \n",
    "    tokenizer, \n",
    "    max_len=MAX_LEN, \n",
    "    threshold=threshold \n",
    ")\n",
    "print(generated_summary_threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
